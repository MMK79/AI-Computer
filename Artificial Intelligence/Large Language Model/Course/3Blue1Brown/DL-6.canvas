{
	"nodes":[
		{"id":"7f90486bad85824f","type":"text","text":"Words (Tokens)","x":-355,"y":-224,"width":250,"height":60},
		{"id":"7086961319015688","type":"text","text":"Vector (Representation)","x":40,"y":-224,"width":250,"height":60},
		{"id":"8c1094901a9e3d5e","type":"text","text":"Directions in this high dimension = Semantic meaning","x":-54,"y":-320,"width":439,"height":50},
		{"id":"ca4ab7b4c4f7b4f0","type":"text","text":"Goal of Transformer  = Adjust embedding\nWe want to capture higher/contextual meaning not meaning of an individual word","x":-220,"y":-100,"width":400,"height":86},
		{"id":"ee6e932800ec6fc3","type":"text","text":"Initial Embedding in Transformer:\nLookup table with no reference to the context (Different meaning have same embedding(not exactly the same cause position is encoded within embedding but you get it))","x":240,"y":-115,"width":485,"height":116},
		{"id":"3bbb543ae811c74c","type":"text","text":"Attention:\nLet surrounding have impact on the word embedding (different meaning of the same word have different embedding base on context)","x":840,"y":-115,"width":434,"height":116},
		{"id":"55c2ad0d32ca276f","type":"text","text":"The last word (the word before the word that we want to predict will encode full context window)","x":1440,"y":-87,"width":400,"height":60},
		{"id":"7011da838b3c8e70","type":"text","text":"Single-Head Attention","x":932,"y":60,"width":250,"height":60},
		{"id":"2b75b5cae39b5007","type":"text","text":"$$Embedded\\ Vectors$$\n$$+$$\n$$(Query\\ for\\ each\\ token) \\times \\mathbf W_Q = Query\\ Vector$$","x":1274,"y":29,"width":489,"height":123},
		{"id":"a79b3e6401b02a8f","type":"text","text":"Query matrix ($\\mathbf W_Q$):\ndo something to our embedding (high dimensional space 12,288) which cause a new vector create in Query/Key space (low dimension space 128) which encode good Query (Think of Query as Question)\n$$\\mathbf E_i . \\mathbf Q_K = \\mathbf Q_i$$\n* Have a smaller dimension\n* Parameter of the model = behavior is learnt from data\n* Hard to understand how it is really working","x":1980,"y":-197,"width":469,"height":280},
		{"id":"0ea48302be499552","type":"text","text":"Keys:\nMatching queries and key when they are close to each other (they are same dimension) $\\xrightarrow{how?}$ Calculate dot product $\\rightarrow$ $\\mathbf Q_i . \\mathbf K_i$  $\\rightarrow$ larger dot product = more align/related (Attend to) $\\rightarrow$ we apply soft-max (Normalize + Probability distribution) on these ($-\\infty, +\\infty$)\nKey matrix ($\\mathbf W_K$):\nthink of key as answer to Queries\n$$\\mathbf E_i . \\mathbf W_K = \\mathbf K_i$$\n* In this case, same dimension as query matrix (128x12,288)","x":1980,"y":144,"width":469,"height":265},
		{"id":"bb0c61cbd8923c4a","type":"text","text":"Attention Pattern:\nGrid of $\\mathbf K_i . \\mathbf Q_i$ ","x":2620,"y":83,"width":179,"height":60},
		{"id":"6b387e0f8262dd4a","type":"text","text":"Attention Mechanism:\n$$Attention(\\mathbf Q, \\mathbf K, \\mathbf V) = softmax(\\frac{\\mathbf K^T \\mathbf Q}{\\sqrt d_k})\\mathbf V$$\n$\\mathbf Q$: Query\n$\\mathbf K$: Key\n$d_k$: Query/Key dimension $\\xrightarrow{for}$ Numerical Stability\n$\\mathbf V$: Value","x":2971,"y":17,"width":398,"height":192},
		{"id":"6aaae513d9ef2841","type":"text","text":"instead of training only on one sentence, when we train we break down sentence, and constantly predicting next word $\\xrightarrow{why?}$ More efficient","x":2880,"y":-120,"width":580,"height":73},
		{"id":"945cbc6e6423df98","type":"text","text":"You don't want later tokens to impact previous token in Attention Pattern $\\rightarrow$ Zero them by hand $\\xrightarrow{problem?}$ You lose probability distribution (numbers won't add up to 1) $\\xrightarrow{solution}$ Before soft-max we set them to $- \\infty$ (we call this Masking)","x":2971,"y":260,"width":618,"height":108},
		{"id":"6692b368386f48dd","type":"text","text":"Attention Pattern Size = (Context Size)\\*\\*2\n* This is the reason that context size is a huge bottle neck\n\t* Techniques to handle this problem:\n\t\t* [[Sparse Attention Mechanism]]\n\t\t* [[Block-wise Attention]]\n\t\t* [[Linformer]]\n\t\t* [[Reformer]]\n\t\t* [[Ring Attention]]\n\t\t* [[Long Former]]\n\t\t* [[Adaptive Attention Span]]\n\t\t* ...","x":2972,"y":409,"width":488,"height":355},
		{"id":"3cf0d94c7ae1e6bd","type":"text","text":" Value Matrix ($\\mathbf W_V$):\n $$\\mathbf E_i . \\mathbf W_V = Value $$\n* Value: Vector that get add to next word\n* Same dimension as your embedding (high dimension, 12,288)\n* If this word is relevant to adjusting meaning of something else, what should get add to that something else in order to reflect this\n* Multiply your attention pattern with Values $\\rightarrow$ Now you add up all these values $\\Delta E$ $\\rightarrow$ You get $\\mathbf E'$","x":3720,"y":-36,"width":540,"height":299},
		{"id":"df59addee9e0b852","type":"text","text":"Query = (128 (Q/K dimension) \\* 12,288(Embedding Dimension))\nKey = (128 (Q/K dimension) \\* 12,288(Embedding Dimension))\nValue = 12,288 \\* 12,288\n* Your Value can be order of magnitude bigger than Query/Key dimensions $\\xrightarrow{But}$ It will be much more efficient if $Value\\ Params = Query\\ Params + Key\\ Params$ $\\rightarrow$ Specially in multiple-head attention running in parallel","x":4440,"y":12,"width":650,"height":204},
		{"id":"b5a4a33269a3939b","type":"text","text":"Breakdown Value into 2 matrices:\n* Overall linear map = Low Rank Transformation\n![[Value-Matrix-Breakdown]]\n* All we said was for self-attention\n* Cross-attention = model that process distinct space data $\\rightarrow$ $\\mathbf K$ and $\\mathbf Q$ act on different sets + No more masking","x":5340,"y":-168,"width":500,"height":564},
		{"id":"2b05b9982a565e9d","type":"text","text":"Multi-headed Attention $\\rightarrow$ Run in parallel + distinct $\\mathbf W_Q, \\mathbf W_K, \\mathbf W_{V\\uparrow}, \\mathbf W_{V{\\downarrow}}$\nGPT use 96 head in each block:\n* 96 different change $\\rightarrow$ sum all of them\n* give model the ability to learn distinct ways/patterns that context change meaning\n* All $value{\\uparrow}$ matrices $\\rightarrow$ staple them together = Output matrix\n* Value matrix = $Value\\downarrow$\n* Deeper you go = deeper meaning/embedding, higher level and more abstract\n* Attentions importance comes from being Parallelizable not that it enables any specific behavior\n\t* Paralleziation enable you to run huge number of computation in short time\n* Big lesson in deep learning $\\rightarrow$ bigger scale, better result","x":5205,"y":500,"width":770,"height":326}
	],
	"edges":[
		{"id":"cbbaa6a204864485","fromNode":"7f90486bad85824f","fromSide":"right","toNode":"7086961319015688","toSide":"left","label":"Embedding"},
		{"id":"3d7a653189449d42","fromNode":"7086961319015688","fromSide":"top","toNode":"8c1094901a9e3d5e","toSide":"bottom"},
		{"id":"516f5cb2aad9e7ad","fromNode":"7f90486bad85824f","fromSide":"right","toNode":"ca4ab7b4c4f7b4f0","toSide":"top"},
		{"id":"b29bcb2aa5943f96","fromNode":"ca4ab7b4c4f7b4f0","fromSide":"right","toNode":"ee6e932800ec6fc3","toSide":"left"},
		{"id":"d506b37dac5467ec","fromNode":"ee6e932800ec6fc3","fromSide":"right","toNode":"3bbb543ae811c74c","toSide":"left","label":"Solution"},
		{"id":"4e5c9b28a6d13718","fromNode":"3bbb543ae811c74c","fromSide":"right","toNode":"55c2ad0d32ca276f","toSide":"left","label":"Explanation"},
		{"id":"a4320ce4908b2076","fromNode":"3bbb543ae811c74c","fromSide":"bottom","toNode":"7011da838b3c8e70","toSide":"top"},
		{"id":"2502e103175b995c","fromNode":"7011da838b3c8e70","fromSide":"right","toNode":"2b75b5cae39b5007","toSide":"left"},
		{"id":"6aa36a453ddfeef8","fromNode":"2b75b5cae39b5007","fromSide":"right","toNode":"a79b3e6401b02a8f","toSide":"left"},
		{"id":"cb848ac4e818bd55","fromNode":"2b75b5cae39b5007","fromSide":"right","toNode":"0ea48302be499552","toSide":"left"},
		{"id":"475ca7429f720722","fromNode":"0ea48302be499552","fromSide":"right","toNode":"bb0c61cbd8923c4a","toSide":"left"},
		{"id":"2a5de162affdbcdf","fromNode":"a79b3e6401b02a8f","fromSide":"right","toNode":"bb0c61cbd8923c4a","toSide":"left"},
		{"id":"e0c305a6a75eddd0","fromNode":"bb0c61cbd8923c4a","fromSide":"right","toNode":"6b387e0f8262dd4a","toSide":"left"},
		{"id":"0aecd74b9f7c503c","fromNode":"6b387e0f8262dd4a","fromSide":"top","toNode":"6aaae513d9ef2841","toSide":"bottom"},
		{"id":"8695e9b5d4d7e28a","fromNode":"bb0c61cbd8923c4a","fromSide":"right","toNode":"945cbc6e6423df98","toSide":"left"},
		{"id":"c36d1bc92429fe5b","fromNode":"bb0c61cbd8923c4a","fromSide":"right","toNode":"6692b368386f48dd","toSide":"left"},
		{"id":"94149f0a52819c41","fromNode":"6b387e0f8262dd4a","fromSide":"right","toNode":"3cf0d94c7ae1e6bd","toSide":"left","label":"How to Update Embedding?\n\n\n"},
		{"id":"63c1b77345e10de8","fromNode":"3cf0d94c7ae1e6bd","fromSide":"right","toNode":"df59addee9e0b852","toSide":"left","label":"Parameters"},
		{"id":"a551c0e3c0f3d003","fromNode":"df59addee9e0b852","fromSide":"right","toNode":"b5a4a33269a3939b","toSide":"left","label":"Value Breakdown"},
		{"id":"3cf10989675499bc","fromNode":"b5a4a33269a3939b","fromSide":"bottom","toNode":"2b05b9982a565e9d","toSide":"top"}
	]
}