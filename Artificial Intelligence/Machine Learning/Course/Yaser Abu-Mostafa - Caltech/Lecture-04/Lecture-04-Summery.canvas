{
	"nodes":[
		{"id":"0191cd2a94c86281","type":"text","text":"$$E(h,f)$$\n$$Pointwise\\ Definition: e(h(x),f(x))$$\nIn-sample Error:\n$$E_{in}(h) = \\frac{1}{N}\\sum_{n=1}^{N} e(h(\\mathbf x_{n}), f(\\mathbf x_{n})) $$\nOut-of-sample Error:\n$$E_{out}(h) = \\mathbb E_{x}[e(h(\\mathbf x), f(\\mathbf x))]$$\n* $x$ is a general point from your input space $X$","x":180,"y":-968,"width":380,"height":357},
		{"id":"46263947b4f7dc9e","type":"file","file":"Artificial Intelligence/Machine Learning/Course/Yaser Abu-Mostafa - Caltech/Lecture-04/Attachments/Penalize-error.md","x":640,"y":-586,"width":400,"height":193},
		{"id":"480a13ebeb71de62","type":"text","text":"* No Merit to Choosing one error function over another\n* Not Analytic Question but Domain Question","x":1080,"y":-564,"width":300,"height":149},
		{"id":"d5f3b0d9376b065a","type":"text","text":"Plausible Measure: measure that analytically have merit.\n* analytic argument have many assumptions\n$$Plausible\\ measures: squared\\ error \\equiv Gaussian\\ noise$$","x":1520,"y":-569,"width":460,"height":159},
		{"id":"55ca2b8f973e00ab","type":"text","text":"Friendly Measure: No Good Justification + Easy to Use\n$$Friendly\\ Measure: closed-form\\ solution, convex\\ optimizatin$$\nNo closed-form solution --> Favorable Optimization","x":1520,"y":-327,"width":484,"height":127},
		{"id":"1f14002f637836d8","type":"text","text":"User give Error measure","x":1520,"y":-720,"width":250,"height":60},
		{"id":"cad24260cdccc956","type":"text","text":"Error Measure","x":-240,"y":-819,"width":250,"height":60},
		{"id":"ce940948c32fadf4","type":"text","text":"Lecture-04","x":-680,"y":-240,"width":250,"height":60},
		{"id":"ca813666012b2c36","type":"text","text":"How to choose the Error Measure","x":180,"y":-519,"width":250,"height":60},
		{"id":"273d3d2401e0b572","type":"text","text":"Learning Diagram with Error Measure","x":180,"y":-180,"width":198,"height":60},
		{"id":"dfed25ae993a4a85","type":"text","text":"1. Evaluate the qualitative statement of $g(x) \\approx f(x)$ --> Grade\n2.  Feeding to Learning algorithm --> try to minimize it","x":891,"y":-212,"width":339,"height":124},
		{"id":"86855b797e8e5eed","type":"text","text":"Noisy Target","x":-280,"y":400,"width":195,"height":60},
		{"id":"4fa83c44c7481c8d","type":"text","text":"Target function is not always a function\nmathematical notion of function: you have to return a unique value for every point in domain","x":40,"y":352,"width":295,"height":157},
		{"id":"d7373d32d8305eba","type":"text","text":"$$Noisy\\ Target = \\textcolor{green}{deterministic\\ target}\\ (\\textcolor{green}{f(x) = \\mathbb E(y \\vert \\mathbf x)})\\ \\textcolor{red}{plus\\ noise}\\ (\\textcolor{red}{y - f(x)})$$","x":360,"y":391,"width":580,"height":80},
		{"id":"52fda60ad86fe5b1","type":"text","text":"deterministic = no randomness","x":508,"y":296,"width":285,"height":56},
		{"id":"3c2806ca2c32d8f9","type":"text","text":"Deterministic target = special case of Probability distribution\n$$P(y\\vert \\mathbf x) is\\ zero\\ expect\\ for\\ y = f(x)$$","x":840,"y":276,"width":288,"height":97},
		{"id":"d87a31eb72252881","type":"text","text":"$(\\mathbf x, y)$ generated by joint distribution (assuming $y$ is conditional of $x$)\n $$P(\\mathbf x)P(y\\vert \\mathbf x)$$","x":300,"y":599,"width":325,"height":110},
		{"id":"d42be3cc789cf22d","type":"text","text":"Target Distribution","x":-280,"y":624,"width":195,"height":60},
		{"id":"96291028e5bba9c1","type":"text","text":"$y = f(x) \\overset{insted\\ of\\ that}{\\longrightarrow} P(y\\ \\vert \\ \\mathbf x)$","x":-6,"y":624,"width":250,"height":60},
		{"id":"56b75d9c1088510a","type":"file","file":"Artificial Intelligence/Machine Learning/Course/Yaser Abu-Mostafa - Caltech/Lecture-04/Attachments/Lecture-04-Final-Leraning-diagram.md","x":724,"y":511,"width":432,"height":286},
		{"id":"30bdad80bfa78d5e","type":"text","text":"Example:\nTarget Distribution: Creditworthiness\nInput Distribution: Population Salary","x":1840,"y":611,"width":322,"height":88},
		{"id":"a34a565897e0b6e0","type":"text","text":"Distinction between $\\mathbf P(y\\vert \\mathbf x)$ and $\\mathbf P(\\mathbf x)$:\n* Both pour in training examples --> joint distribution of them generate training data $\\mathbf P(y\\vert \\mathbf x)\\ \\&\\ \\mathbf P(\\mathbf x)\\ \\overset{Merge}{\\longrightarrow} P(\\mathbf x, y)$\n* Both Unknown --> Input Distribution is just for Hoeffding (No other assumption about it just let it be there) - Target Distribution general statement for Machine learning\n* You try to learn Target Distribution - When the Input Distribution just Quantifies relative importance of $x$ (it will impact your final hypothesis but you are not trying to learn it)","x":1240,"y":499,"width":535,"height":311},
		{"id":"4521326bb78f7e05","type":"text","text":"Preamble of Theory","x":-284,"y":996,"width":203,"height":60},
		{"id":"caf40e2b41aa2892","type":"text","text":"Learning is feasible in a probabilistic sense --> by stating that the out-of-sample performance is close to in-sample performance","x":-17,"y":957,"width":319,"height":139},
		{"id":"872a69d917775971","type":"text","text":"Lecture 02:\n$\\nu \\approx \\mu \\implies \\mu \\approx \\nu$","x":56,"y":1146,"width":172,"height":60},
		{"id":"c153add43e3ce4c6","type":"text","text":"learning means you get the hypothesis right. Condition of learning = $g \\approx f$, even in term of error it should be $E_{out}(g) \\approx 0$ NOT $E_{out}(g) \\approx E_{in}(g)$ because out-of-sample error measure how far you are from the target function over the entire space. ","x":358,"y":920,"width":365,"height":213},
		{"id":"14725eb5b01009c5","type":"text","text":"$E_{out}(g) \\approx E_{in}(g)$:\n* Generalization\n* Important building block\n* We don't have access to $E_{out}$\n* $E_(in)$ is a proxy for $E_{out}$","x":788,"y":929,"width":305,"height":194},
		{"id":"f14877c73b9e01fb","type":"text","text":"The 2 Questions of Learning\n1. Can we make sure that $E_{out}(g)$ is close enough to $E_{in}(g)$?\n2. Can we make the $E_{in}(g)$ small enough?\n","x":1190,"y":921,"width":330,"height":210},
		{"id":"4182e3a2ae99b240","type":"file","file":"Artificial Intelligence/Machine Learning/Course/Yaser Abu-Mostafa - Caltech/Lecture-04/Attachments/What-theory-achieve.png","x":1575,"y":914,"width":400,"height":225},
		{"id":"a48c88f06b0ab165","type":"text","text":"Error measure\n$$E_{in}(h) = \\frac{1}{N}\\sum_{n=1}^{N} e(h(\\mathbf x_{n}), f(\\mathbf x_{n})) $$\nFinal Diagram of learning\nInput Distribution vs Target Distribution\nNoisy Target","x":-694,"y":-120,"width":279,"height":245},
		{"id":"e43fd7f1a2580c34","type":"file","file":"Artificial Intelligence/Machine Learning/Course/Yaser Abu-Mostafa - Caltech/Lecture-04/Attachments/Lecture-04-Learning-diagram.md","x":660,"y":-924,"width":460,"height":269},
		{"id":"6db756d8fa939d2d","type":"file","file":"Artificial Intelligence/Machine Learning/Course/Yaser Abu-Mostafa - Caltech/Lecture-04/Attachments/Lecture-04-Leraning-diagram-error-measure.md","x":430,"y":-327,"width":400,"height":353}
	],
	"edges":[
		{"id":"eb24a6ff4b5c35a2","fromNode":"ce940948c32fadf4","fromSide":"right","toNode":"cad24260cdccc956","toSide":"left"},
		{"id":"41037eaa237efffc","fromNode":"cad24260cdccc956","fromSide":"right","toNode":"0191cd2a94c86281","toSide":"left"},
		{"id":"33636e72916d0f25","fromNode":"ca813666012b2c36","fromSide":"right","toNode":"46263947b4f7dc9e","toSide":"left"},
		{"id":"a7bdecd9fb497719","fromNode":"cad24260cdccc956","fromSide":"right","toNode":"ca813666012b2c36","toSide":"left"},
		{"id":"736092fb58c76d4c","fromNode":"46263947b4f7dc9e","fromSide":"right","toNode":"480a13ebeb71de62","toSide":"left"},
		{"id":"fa17c7680eb8a224","fromNode":"480a13ebeb71de62","fromSide":"right","toNode":"1f14002f637836d8","toSide":"left"},
		{"id":"83cacd30e282f906","fromNode":"480a13ebeb71de62","fromSide":"right","toNode":"d5f3b0d9376b065a","toSide":"left"},
		{"id":"e8bbec275c435a93","fromNode":"480a13ebeb71de62","fromSide":"right","toNode":"55ca2b8f973e00ab","toSide":"left"},
		{"id":"fee17a6c24aebdf0","fromNode":"cad24260cdccc956","fromSide":"right","toNode":"273d3d2401e0b572","toSide":"left"},
		{"id":"d024ff5f5f80fff3","fromNode":"ce940948c32fadf4","fromSide":"right","toNode":"86855b797e8e5eed","toSide":"left"},
		{"id":"3d039e2ac8310962","fromNode":"86855b797e8e5eed","fromSide":"right","toNode":"4fa83c44c7481c8d","toSide":"left"},
		{"id":"bbc5497126ee3df3","fromNode":"86855b797e8e5eed","fromSide":"bottom","toNode":"d42be3cc789cf22d","toSide":"top"},
		{"id":"5a0015f2a64fefcc","fromNode":"ce940948c32fadf4","fromSide":"right","toNode":"d42be3cc789cf22d","toSide":"left"},
		{"id":"3c7f73f83991709c","fromNode":"d42be3cc789cf22d","fromSide":"right","toNode":"96291028e5bba9c1","toSide":"left"},
		{"id":"2a2fcdd92a9761f9","fromNode":"96291028e5bba9c1","fromSide":"right","toNode":"d87a31eb72252881","toSide":"left"},
		{"id":"1481712e04ca94c4","fromNode":"4fa83c44c7481c8d","fromSide":"right","toNode":"d7373d32d8305eba","toSide":"left"},
		{"id":"2becb6fa6768c697","fromNode":"d7373d32d8305eba","fromSide":"top","toNode":"52fda60ad86fe5b1","toSide":"bottom"},
		{"id":"7badd354ea7842e8","fromNode":"52fda60ad86fe5b1","fromSide":"right","toNode":"3c2806ca2c32d8f9","toSide":"left"},
		{"id":"694450a79ab9e54b","fromNode":"d87a31eb72252881","fromSide":"right","toNode":"56b75d9c1088510a","toSide":"left"},
		{"id":"1010d30adbce703c","fromNode":"56b75d9c1088510a","fromSide":"right","toNode":"a34a565897e0b6e0","toSide":"left"},
		{"id":"9540cdecffefc492","fromNode":"a34a565897e0b6e0","fromSide":"right","toNode":"30bdad80bfa78d5e","toSide":"left"},
		{"id":"6278e25bb590c39c","fromNode":"ce940948c32fadf4","fromSide":"right","toNode":"4521326bb78f7e05","toSide":"left"},
		{"id":"08115d665f4e1cd2","fromNode":"4521326bb78f7e05","fromSide":"right","toNode":"caf40e2b41aa2892","toSide":"left"},
		{"id":"6b5b3e05a2d86822","fromNode":"872a69d917775971","fromSide":"top","toNode":"caf40e2b41aa2892","toSide":"bottom"},
		{"id":"8775d9889db04b8f","fromNode":"caf40e2b41aa2892","fromSide":"right","toNode":"c153add43e3ce4c6","toSide":"left"},
		{"id":"e008ba3972807e1b","fromNode":"c153add43e3ce4c6","fromSide":"right","toNode":"14725eb5b01009c5","toSide":"left"},
		{"id":"424111975b9d4d43","fromNode":"14725eb5b01009c5","fromSide":"right","toNode":"f14877c73b9e01fb","toSide":"left"},
		{"id":"51f497b8c8522bf5","fromNode":"f14877c73b9e01fb","fromSide":"right","toNode":"4182e3a2ae99b240","toSide":"left"},
		{"id":"69a208149eaa3dfb","fromNode":"ce940948c32fadf4","fromSide":"bottom","toNode":"a48c88f06b0ab165","toSide":"top"},
		{"id":"9076f6de82639026","fromNode":"0191cd2a94c86281","fromSide":"right","toNode":"e43fd7f1a2580c34","toSide":"left"},
		{"id":"d88c380f2c3842d7","fromNode":"273d3d2401e0b572","fromSide":"right","toNode":"6db756d8fa939d2d","toSide":"left"},
		{"id":"dacf411c1bcb0166","fromNode":"6db756d8fa939d2d","fromSide":"right","toNode":"dfed25ae993a4a85","toSide":"left"}
	]
}