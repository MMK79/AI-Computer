{
	"nodes":[
		{"id":"b3fcdfc36a2f9f15","type":"group","x":520,"y":67,"width":3420,"height":641,"label":"Transformer Layers"},
		{"id":"3298f73ef1efc466","type":"text","text":"Transformer is an encoder-decoder model >> RNN","x":-403,"y":-184,"width":419,"height":64},
		{"id":"2de0f68b3e203915","type":"text","text":"It takes advantage of parallization  of GPU/TPU","x":160,"y":-180,"width":250,"height":60},
		{"id":"3d9197c7d100382f","type":"text","text":"Process much more data in same amount of time","x":640,"y":-179,"width":232,"height":60},
		{"id":"376ff04ca3e61056","type":"text","text":"Process all tokens at once","x":640,"y":-60,"width":250,"height":60},
		{"id":"dfc2651d2e238577","type":"text","text":"It is not simply one encoder, but it is stacks of encoders/decoders","x":540,"y":87,"width":291,"height":100},
		{"id":"f2f291ab5fdd7638","type":"text","text":"Capture meaning of a word in its context","x":1060,"y":-60,"width":250,"height":60},
		{"id":"ff7b6d0704da5921","type":"text","text":"it is a hyper-parameter","x":1040,"y":107,"width":216,"height":60},
		{"id":"a14a4469570380b1","type":"text","text":"Encoders have an identical(similar in every details) structure but have different weights","x":1040,"y":380,"width":368,"height":100},
		{"id":"d838c7a1b9f1f0b5","type":"text","text":"Decoder Component:\nSelf Attention + Encoder-Decoder Attention + FeedForward Neural Network","x":1040,"y":608,"width":597,"height":80},
		{"id":"7dc4ff8dd8ae0043","type":"text","text":"In original paper it was 6","x":1340,"y":107,"width":228,"height":60},
		{"id":"433f8c0618eb9850","type":"text","text":"Stacks of encoder and decoders should contain same amount of number $\\xrightarrow{means}$ if you have 6 encoder you must have 6 decoder","x":1340,"y":240,"width":377,"height":120},
		{"id":"3615f03ba7e10025","type":"text","text":"Each encoder breaks into 2 sublayers:\nSelf Attention + FeedForward Neural Network","x":1600,"y":392,"width":380,"height":77},
		{"id":"4d8b3678b96c9368","type":"text","text":"Encoder-Decoder Attention:\nHelp decoder focus on relevant sentence","x":1731,"y":614,"width":349,"height":68},
		{"id":"46032e653ac465b3","x":85,"y":63,"width":400,"height":148,"type":"file","file":"Artificial Intelligence/Large Language Model/Concept/Transformers/BERT/Attachments/Basic-Transformer.md"},
		{"id":"7fd46f52b24ddfe4","type":"text","text":"Self attention:\nHelps Encoder looks at the relevant parts of the words as it encode center words in the input sentence\nfor more detail --> [[DL-6.canvas|DL-6]]","x":2080,"y":261,"width":443,"height":116},
		{"id":"2c600e16ee018329","type":"text","text":"Feed Forward Neural Network:\noutput of self-attention is fed to feed forward NN\n* The exact same FFNN is independently apply to each position","x":2080,"y":469,"width":528,"height":139},
		{"id":"6b14f788bc29aff7","type":"text","text":"* Encoder-Decoder $\\rightarrow$ BART\n* Decoder Only $\\rightarrow$ GPT\n* Encoder Only $\\rightarrow$ BERT","x":2080,"y":820,"width":296,"height":131},
		{"id":"08bf68a90dbbe9f9","type":"text","text":"BERT (2010):\nBidirectional Encoder Representation from Transformers","x":2440,"y":838,"width":458,"height":95},
		{"id":"8b2d00b184e56d21","type":"text","text":"Input embedding break into Query, Key, Value vectors","x":2608,"y":294,"width":439,"height":50},
		{"id":"76886738ab644ef2","type":"text","text":"All these computation computed by using happen in parallel\nWeights that the transformer learned in the form of matrix during learning process computation","x":3177,"y":282,"width":743,"height":86},
		{"id":"db19a6844f4fc0fc","type":"text","text":"$sotfmax(\\frac{\\mathbf Q.\\mathbf K^T}{\\sqrt d}).\\mathbf V = \\mathbf Z$ \n* First part capture intuition = keep intact the values of the word you want to focus on and leave leave out irrelevant words by multiplying them with tiny numbers\n* After it apply and produce self-attention layer in will send them to Feed Forward Neural Network","x":3310,"y":420,"width":477,"height":220},
		{"id":"dc719c27c1b2c8d3","type":"text","text":"2 variation:\n* Contain BERT base which had 12 stacks of transformer (approximately 110 million parameters), 786 FFN (hidden units), 12 attention head\n* BERT Large which had 24 layers of transformer and 314 million parameters, 1024 (hidden units), 15 attention head\n* Transformer, 6 transformer layer, 512 FFNN (hidden layer), 8 attention head","x":3164,"y":729,"width":451,"height":314},
		{"id":"e85e046695507baf","type":"text","text":"Hardware: TPU\nData: Entire Wikipedia and BookEorpus","x":3164,"y":1080,"width":292,"height":60},
		{"id":"251accb4573608f1","type":"text","text":"One million steps (update model parameters after mini-batch of data)","x":3164,"y":1180,"width":356,"height":77},
		{"id":"d6457f0bebed4d3f","type":"text","text":"Multi-task object $\\xrightarrow{why?}$ cause it works on both sentence level and token level","x":3164,"y":1305,"width":356,"height":80},
		{"id":"eac33e54d05d5fa5","type":"text","text":"Masked language model (MLM):\nSentence were masked, and model tried to predict mask word\n* To train BERT from scratch you have to mask a certain percentage on your corpus which 15% is recommended $\\xrightarrow{balance\\ between}$ Mask $\\downarrow$ = Cost$\\uparrow$ & Mask$\\uparrow$ = Context$\\downarrow$","x":3625,"y":1110,"width":499,"height":175},
		{"id":"652631cfa1f5ccc1","type":"text","text":"handle long input context","x":3760,"y":856,"width":250,"height":60},
		{"id":"aa4e6977844dfec3","type":"text","text":"Solve the NLP tasks that involve test classification as well","x":4320,"y":1300,"width":283,"height":86},
		{"id":"4b4a39aeae55c862","type":"text","text":"Next sentence prediction (NPS) - Binary Classification Task:\nLearn relation between sentences and predict next sentences given the first one","x":3625,"y":1430,"width":631,"height":83},
		{"id":"c515417321575eef","type":"text","text":"input sentence embedding:\n* Token embedding\n* Segment embedding\n* Position embedding","x":3164,"y":1513,"width":256,"height":145},
		{"id":"5051fa8c6f2f4cc8","type":"text","text":"Concatenate pairs of text then fed into model ","x":4760,"y":1313,"width":224,"height":60},
		{"id":"2d24287f6635e6c9","type":"text","text":"learn the order of the words","x":4760,"y":1470,"width":253,"height":50},
		{"id":"fb1536bfe410cd2e","type":"text","text":"* Single Sentence Classification\n* Sentence Pair Classification\n* Q&A\n* Single Sentence Tagging Task","x":4760,"y":1600,"width":300,"height":129},
		{"id":"d8f03ae073b7d09e","type":"text","text":"BERT consist of stacks of transformers which designed to process input sequence up to 512 length\nOrder of this input sequence is incorporate into position embedding $\\xrightarrow{Which\\ enable}$ BERT to learn vector representation for each position","x":5160,"y":1420,"width":485,"height":151},
		{"id":"dafdd4fd728c90ea","type":"text","text":"Use segment embedding\nSpecial Token SEP that separate two different splits of a sentence","x":5280,"y":1300,"width":289,"height":86},
		{"id":"5676e41302e25943","x":4203,"y":282,"width":400,"height":213,"type":"file","file":"Artificial Intelligence/Large Language Model/Concept/Transformers/BERT/Attachments/Encoder.md"}
	],
	"edges":[
		{"id":"61c4dd277588cfd8","fromNode":"3298f73ef1efc466","fromSide":"right","toNode":"2de0f68b3e203915","toSide":"left","label":"Because"},
		{"id":"3b7e15bf75917c32","fromNode":"2de0f68b3e203915","fromSide":"right","toNode":"3d9197c7d100382f","toSide":"left","label":"Which help you"},
		{"id":"603798d1852beb7e","fromNode":"2de0f68b3e203915","fromSide":"right","toNode":"376ff04ca3e61056","toSide":"left","label":"More importantly"},
		{"id":"31c013c3908e8762","fromNode":"376ff04ca3e61056","fromSide":"right","toNode":"f2f291ab5fdd7638","toSide":"left","label":"Which help"},
		{"id":"67aa28863742b57a","fromNode":"dfc2651d2e238577","fromSide":"right","toNode":"ff7b6d0704da5921","toSide":"left","label":"how many?"},
		{"id":"07a0efef2382abf0","fromNode":"ff7b6d0704da5921","fromSide":"right","toNode":"7dc4ff8dd8ae0043","toSide":"left"},
		{"id":"83b628fd1c03567e","fromNode":"ff7b6d0704da5921","fromSide":"right","toNode":"433f8c0618eb9850","toSide":"left","label":"Must important thing"},
		{"id":"54dc48064a440fef","fromNode":"dfc2651d2e238577","fromSide":"bottom","toNode":"a14a4469570380b1","toSide":"left","label":"Encoders"},
		{"id":"3ffc94dfa4c033a1","fromNode":"a14a4469570380b1","fromSide":"right","toNode":"3615f03ba7e10025","toSide":"left"},
		{"id":"333756e2ef9a3bc3","fromNode":"3615f03ba7e10025","fromSide":"right","toNode":"7fd46f52b24ddfe4","toSide":"left"},
		{"id":"5361bc76036ac788","fromNode":"3615f03ba7e10025","fromSide":"right","toNode":"2c600e16ee018329","toSide":"left"},
		{"id":"8275ee10df300bd0","fromNode":"dfc2651d2e238577","fromSide":"bottom","toNode":"d838c7a1b9f1f0b5","toSide":"left","label":"Decoders"},
		{"id":"759a0e3c2f6473eb","fromNode":"d838c7a1b9f1f0b5","fromSide":"right","toNode":"4d8b3678b96c9368","toSide":"left"},
		{"id":"8304d395ee0b1054","fromNode":"7fd46f52b24ddfe4","fromSide":"right","toNode":"8b2d00b184e56d21","toSide":"left"},
		{"id":"69310dc42974f406","fromNode":"8b2d00b184e56d21","fromSide":"right","toNode":"76886738ab644ef2","toSide":"left","label":"Which"},
		{"id":"0f58916321472ed7","fromNode":"8b2d00b184e56d21","fromSide":"right","toNode":"db19a6844f4fc0fc","toSide":"left"},
		{"id":"275b69c4e9a42c8a","fromNode":"76886738ab644ef2","fromSide":"bottom","toNode":"db19a6844f4fc0fc","toSide":"top"},
		{"id":"a192a40908e3f12e","fromNode":"b3fcdfc36a2f9f15","fromSide":"bottom","toNode":"6b14f788bc29aff7","toSide":"top","label":"Variation of Transformers"},
		{"id":"fdf449ebbd53cab3","fromNode":"6b14f788bc29aff7","fromSide":"right","toNode":"08bf68a90dbbe9f9","toSide":"left"},
		{"id":"7ddd1fccee6428a3","fromNode":"08bf68a90dbbe9f9","fromSide":"right","toNode":"dc719c27c1b2c8d3","toSide":"left","label":"Trained in"},
		{"id":"8ae04633e803f0cf","fromNode":"08bf68a90dbbe9f9","fromSide":"right","toNode":"e85e046695507baf","toSide":"left","label":"Trained on"},
		{"id":"56ec4c532c56ed4a","fromNode":"08bf68a90dbbe9f9","fromSide":"right","toNode":"251accb4573608f1","toSide":"left","label":"Trained For"},
		{"id":"7ed3f42dc3acf7c4","fromNode":"08bf68a90dbbe9f9","fromSide":"right","toNode":"d6457f0bebed4d3f","toSide":"left","label":"Targeted at"},
		{"id":"7f740694bf695cc6","fromNode":"dc719c27c1b2c8d3","fromSide":"right","toNode":"652631cfa1f5ccc1","toSide":"left","label":"Enable"},
		{"id":"e7379054f13b26ee","fromNode":"d6457f0bebed4d3f","fromSide":"right","toNode":"eac33e54d05d5fa5","toSide":"left"},
		{"id":"8222dfebf04a0f15","fromNode":"d6457f0bebed4d3f","fromSide":"right","toNode":"4b4a39aeae55c862","toSide":"left"},
		{"id":"be2d0bfe506d2666","fromNode":"08bf68a90dbbe9f9","fromSide":"right","toNode":"c515417321575eef","toSide":"left","label":"to train BERT\ndifferent type of\nEmbedding"},
		{"id":"c77025a1aac1ff7b","fromNode":"d6457f0bebed4d3f","fromSide":"right","toNode":"aa4e6977844dfec3","toSide":"left"},
		{"id":"34ca3263ca64b799","fromNode":"aa4e6977844dfec3","fromSide":"right","toNode":"5051fa8c6f2f4cc8","toSide":"left","label":"How?"},
		{"id":"e6d7f70148847c6e","fromNode":"5051fa8c6f2f4cc8","fromSide":"right","toNode":"dafdd4fd728c90ea","toSide":"left","label":"How to distinguish\ninput in given pairs"},
		{"id":"8721b67526263468","fromNode":"aa4e6977844dfec3","fromSide":"right","toNode":"2d24287f6635e6c9","toSide":"left","label":"Problem"},
		{"id":"5b77836817f06fbe","fromNode":"2d24287f6635e6c9","fromSide":"right","toNode":"d8f03ae073b7d09e","toSide":"left","label":"Solution"},
		{"id":"c8e783f00e5eced1","fromNode":"aa4e6977844dfec3","fromSide":"bottom","toNode":"fb1536bfe410cd2e","toSide":"left","label":"Other\nDownstream\nTasks"},
		{"id":"a5a54c71c34ed943","fromNode":"46032e653ac465b3","fromSide":"right","toNode":"dfc2651d2e238577","toSide":"left"},
		{"id":"b53dee3d0afd92c6","fromNode":"3298f73ef1efc466","fromSide":"bottom","toNode":"46032e653ac465b3","toSide":"left"},
		{"id":"854d41759eaba0de","fromNode":"b3fcdfc36a2f9f15","fromSide":"right","toNode":"5676e41302e25943","toSide":"left"}
	]
}